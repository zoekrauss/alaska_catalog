{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick code to read in station information from all picks in the Alaska QuakeML files, and save it to xml for much faster reading later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time; import datetime\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from collections import defaultdict\n",
    "import geopandas as gpd\n",
    "from obspy import read_inventory\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in information from all quakeml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['catalog_css/2018_12/XO_2018_12.quakeml', 'catalog_css/2019_06/XO_2019_06.quakeml', 'catalog_css/2019_01/XO_2019_01.quakeml', 'catalog_css/2019_07/XO_2019_07.quakeml', 'catalog_css/2018_07/XO_2018_07.quakeml', 'catalog_css/2018_09/XO_2018_09.quakeml', 'catalog_css/2018_08/XO_2018_08.quakeml', 'catalog_css/2018_06/XO_2018_06.quakeml', 'catalog_css/2018_11/XO_2018_11.quakeml', 'catalog_css/2018_10/XO_2018_10.quakeml', 'catalog_css/2019_02/XO_2019_02.quakeml', 'catalog_css/2019_05/XO_2019_05.quakeml', 'catalog_css/2019_04/XO_2019_04.quakeml', 'catalog_css/2019_03/XO_2019_03.quakeml', 'catalog_css/2018_05/XO_2018_05.quakeml']\n"
     ]
    }
   ],
   "source": [
    "ml_files = glob.glob('catalog_css/**/XO_*.quakeml')\n",
    "print(ml_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIS TAKES ~ hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/quakeflow/lib/python3.9/site-packages/obspy/io/quakeml/core.py:184: UserWarning: Could not convert 2018-06-18T21:56:60.000Z to type <class 'obspy.core.utcdatetime.UTCDateTime'>. Returning None.\n",
      "  warnings.warn(msg % (text, convert_to))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/quakeflow/lib/python3.9/site-packages/obspy/io/quakeml/core.py:184: UserWarning: Could not convert 2018-11-14T00:42:60.000Z to type <class 'obspy.core.utcdatetime.UTCDateTime'>. Returning None.\n",
      "  warnings.warn(msg % (text, convert_to))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/quakeflow/lib/python3.9/site-packages/obspy/io/quakeml/core.py:184: UserWarning: Could not convert 2020-11-19T01:12:60.000Z to type <class 'obspy.core.utcdatetime.UTCDateTime'>. Returning None.\n",
      "  warnings.warn(msg % (text, convert_to))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/quakeflow/lib/python3.9/site-packages/obspy/io/quakeml/core.py:184: UserWarning: Could not convert 2021-02-09T02:50:60.000Z to type <class 'obspy.core.utcdatetime.UTCDateTime'>. Returning None.\n",
      "  warnings.warn(msg % (text, convert_to))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/quakeflow/lib/python3.9/site-packages/obspy/io/quakeml/core.py:184: UserWarning: Could not convert 2021-05-11T20:56:60.000Z to type <class 'obspy.core.utcdatetime.UTCDateTime'>. Returning None.\n",
      "  warnings.warn(msg % (text, convert_to))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "cat = obspy.core.event.Catalog()\n",
    "for file in ml_files:\n",
    "    count += 1\n",
    "    print(count)\n",
    "    cat.extend(obspy.core.event.read_events(file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of all station-channel pairs that have picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = [p.picks for p in cat.events]\n",
    "picks = sum(picks,[])\n",
    "\n",
    "networks = [p.waveform_id.network_code for p in picks]\n",
    "stations = [p.waveform_id.station_code for p in picks]\n",
    "channels = [p.waveform_id.channel_code[0:2] + '*' for p in picks]\n",
    "# Toss pressure channels:\n",
    "channelToRemove = 'HD*'\n",
    "channels = [value for value in channels if value != channelToRemove]\n",
    "\n",
    "sta_list = [f\"{n}.{s}..{c[0:2]}\" for n, s, c in zip(networks,stations,channels)]\n",
    "sta_list = np.unique(sta_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import read_inventory\n",
    "sta_metadata.write(\"alaska_stations.xml\",\n",
    "                format=\"STATIONXML\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the info for those stations from IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = \",\".join((np.unique(networks)).tolist())\n",
    "channel = \",\".join((np.unique(channels)).tolist())\n",
    "station = \",\".join((np.unique(stations)).tolist())\n",
    "\n",
    "origins = [p.origins for p in cat.events]\n",
    "times = [p[0].time for p in origins]\n",
    "starttime = np.min(times)\n",
    "endtime = np.max(times)\n",
    "\n",
    "sta_metadata = Client(\"iris\").get_stations(starttime=starttime,endtime=endtime,network=network,channel=channel,station=station,location='',level='response')\n",
    "sta_dict = {'network':network,'channel':channel,'station':station}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = [p.origins for p in cat.events]\n",
    "times = [p[0].time for p in origins]\n",
    "starttime = np.min(times)\n",
    "endtime = np.max(times)\n",
    "\n",
    "sta_metadata = Client(\"iris\").get_stations(starttime=starttime,endtime=endtime,network=network,channel=channel,station=station,location='',level='response')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the station inventory to xml file for safekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>elevation(m)</th>\n",
       "      <th>component</th>\n",
       "      <th>response</th>\n",
       "      <th>unit</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AK.ANM..BH</th>\n",
       "      <td>-165.373200</td>\n",
       "      <td>64.564600</td>\n",
       "      <td>338.0</td>\n",
       "      <td>E,N,Z</td>\n",
       "      <td>488760000.00,488760000.00,488760000.00</td>\n",
       "      <td>m/s</td>\n",
       "      <td>AK.ANM..BH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK.ATKA..BH</th>\n",
       "      <td>-174.197495</td>\n",
       "      <td>52.201599</td>\n",
       "      <td>55.0</td>\n",
       "      <td>E,N,Z</td>\n",
       "      <td>488760000.00,488760000.00,488760000.00</td>\n",
       "      <td>m/s</td>\n",
       "      <td>AK.ATKA..BH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK.ATKA..BN</th>\n",
       "      <td>-174.197495</td>\n",
       "      <td>52.201599</td>\n",
       "      <td>55.0</td>\n",
       "      <td>E,N,Z</td>\n",
       "      <td>213760.00,213760.00,213760.00</td>\n",
       "      <td>m/s**2</td>\n",
       "      <td>AK.ATKA..BN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK.ATKA..HN</th>\n",
       "      <td>-174.197495</td>\n",
       "      <td>52.201599</td>\n",
       "      <td>55.0</td>\n",
       "      <td>E,N,Z</td>\n",
       "      <td>213760.00,213760.00,213760.00</td>\n",
       "      <td>m/s**2</td>\n",
       "      <td>AK.ATKA..HN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AK.BAGL..BH</th>\n",
       "      <td>-142.091507</td>\n",
       "      <td>60.489601</td>\n",
       "      <td>1470.0</td>\n",
       "      <td>E,N,Z</td>\n",
       "      <td>488760000.00,488760000.00,488760000.00</td>\n",
       "      <td>m/s</td>\n",
       "      <td>AK.BAGL..BH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XV.FAPT..HH</th>\n",
       "      <td>-149.083100</td>\n",
       "      <td>64.549800</td>\n",
       "      <td>111.2</td>\n",
       "      <td>E,N,Z</td>\n",
       "      <td>503939000.00,503939000.00,503939000.00</td>\n",
       "      <td>m/s</td>\n",
       "      <td>XV.FAPT..HH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XV.FNN1..HH</th>\n",
       "      <td>-149.217800</td>\n",
       "      <td>64.571600</td>\n",
       "      <td>110.0</td>\n",
       "      <td>E,N,Z</td>\n",
       "      <td>503939000.00,503939000.00,503939000.00</td>\n",
       "      <td>m/s</td>\n",
       "      <td>XV.FNN1..HH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XV.FNN2..HH</th>\n",
       "      <td>-149.445600</td>\n",
       "      <td>64.575600</td>\n",
       "      <td>134.6</td>\n",
       "      <td>E,N,Z</td>\n",
       "      <td>503939000.00,503939000.00,503939000.00</td>\n",
       "      <td>m/s</td>\n",
       "      <td>XV.FNN2..HH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XV.FPAP..HH</th>\n",
       "      <td>-149.099200</td>\n",
       "      <td>64.613000</td>\n",
       "      <td>105.5</td>\n",
       "      <td>E,N,Z</td>\n",
       "      <td>503939000.00,503939000.00,503939000.00</td>\n",
       "      <td>m/s</td>\n",
       "      <td>XV.FPAP..HH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XV.FTGH..HH</th>\n",
       "      <td>-148.827900</td>\n",
       "      <td>64.691700</td>\n",
       "      <td>284.5</td>\n",
       "      <td>E,N,Z</td>\n",
       "      <td>503939000.00,503939000.00,503939000.00</td>\n",
       "      <td>m/s</td>\n",
       "      <td>XV.FTGH..HH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>747 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              longitude   latitude  elevation(m) component  \\\n",
       "AK.ANM..BH  -165.373200  64.564600         338.0     E,N,Z   \n",
       "AK.ATKA..BH -174.197495  52.201599          55.0     E,N,Z   \n",
       "AK.ATKA..BN -174.197495  52.201599          55.0     E,N,Z   \n",
       "AK.ATKA..HN -174.197495  52.201599          55.0     E,N,Z   \n",
       "AK.BAGL..BH -142.091507  60.489601        1470.0     E,N,Z   \n",
       "...                 ...        ...           ...       ...   \n",
       "XV.FAPT..HH -149.083100  64.549800         111.2     E,N,Z   \n",
       "XV.FNN1..HH -149.217800  64.571600         110.0     E,N,Z   \n",
       "XV.FNN2..HH -149.445600  64.575600         134.6     E,N,Z   \n",
       "XV.FPAP..HH -149.099200  64.613000         105.5     E,N,Z   \n",
       "XV.FTGH..HH -148.827900  64.691700         284.5     E,N,Z   \n",
       "\n",
       "                                           response    unit           id  \n",
       "AK.ANM..BH   488760000.00,488760000.00,488760000.00     m/s   AK.ANM..BH  \n",
       "AK.ATKA..BH  488760000.00,488760000.00,488760000.00     m/s  AK.ATKA..BH  \n",
       "AK.ATKA..BN           213760.00,213760.00,213760.00  m/s**2  AK.ATKA..BN  \n",
       "AK.ATKA..HN           213760.00,213760.00,213760.00  m/s**2  AK.ATKA..HN  \n",
       "AK.BAGL..BH  488760000.00,488760000.00,488760000.00     m/s  AK.BAGL..BH  \n",
       "...                                             ...     ...          ...  \n",
       "XV.FAPT..HH  503939000.00,503939000.00,503939000.00     m/s  XV.FAPT..HH  \n",
       "XV.FNN1..HH  503939000.00,503939000.00,503939000.00     m/s  XV.FNN1..HH  \n",
       "XV.FNN2..HH  503939000.00,503939000.00,503939000.00     m/s  XV.FNN2..HH  \n",
       "XV.FPAP..HH  503939000.00,503939000.00,503939000.00     m/s  XV.FPAP..HH  \n",
       "XV.FTGH..HH  503939000.00,503939000.00,503939000.00     m/s  XV.FTGH..HH  \n",
       "\n",
       "[747 rows x 7 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sta_metadata.write(\"alaska_stations.xml\",\n",
    "                format=\"STATIONXML\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read it back in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "# round trip\n",
    "%time\n",
    "sta_metadata = read_inventory('alaska_stations.xml',format='STATIONXML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_locs = defaultdict(dict)\n",
    "for network in sta_metadata:\n",
    "    for station in network:\n",
    "        for chn in station:\n",
    "            sid = f\"{network.code}.{station.code}.{chn.location_code}.{chn.code[:-1]}\" + chn.start_date.strftime('%Y%j')\n",
    "            if sid in station_locs:\n",
    "                station_locs[sid][\"component\"] += f\",{chn.code[-1]}\"\n",
    "                station_locs[sid][\"response\"] += f\",{chn.response.instrument_sensitivity.value:.2f}\"\n",
    "            else:\n",
    "                component = f\"{chn.code[-1]}\"\n",
    "                response = f\"{chn.response.instrument_sensitivity.value:.2f}\"\n",
    "                dtype = chn.response.instrument_sensitivity.input_units.lower()\n",
    "                tmp_dict = {}\n",
    "                tmp_dict[\"longitude\"], tmp_dict[\"latitude\"], tmp_dict[\"elevation(m)\"] = (\n",
    "                    chn.longitude,\n",
    "                    chn.latitude,\n",
    "                    chn.elevation,\n",
    "                )\n",
    "                tmp_dict[\"component\"], tmp_dict[\"response\"], tmp_dict[\"unit\"] = component, response, dtype\n",
    "                tmp_dict[\"start_date\"], tmp_dict[\"end_date\"] = chn.start_date,chn.end_date\n",
    "                tmp_dict[\"network\"], tmp_dict[\"station\"] = network.code, station.code\n",
    "                station_locs[sid] = tmp_dict\n",
    "\n",
    "station_locs = pd.DataFrame.from_dict(station_locs,orient='index')\n",
    "station_locs[\"id\"] = station_locs.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do a bit of clean-up... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop stations with < 3 components\n",
    "\n",
    "station_locs = station_locs[(station_locs['component'].str.len()>=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the one station that comes in weird\n",
    "\n",
    "new_comp = station_locs.loc['XO.ET18..HH2018161','component'][4:9]\n",
    "new_resp = ','.join(station_locs.loc['XO.ET18..HH2018161','response'].split(',')[2:5])\n",
    "station_locs.loc['XO.ET18..HH2018161','component'] = new_comp\n",
    "station_locs.loc['XO.ET18..HH2018161','response'] = new_resp\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last thing: remove the date from \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_locs['id']=station_locs['id'].str.slice(stop=-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All done! Now let's save it as a parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pq/xg5dptzs1cb1dsy5gzw5cm7h0000gn/T/ipykernel_8120/1307806723.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstation_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/quakeflow/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/quakeflow/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2675\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2677\u001b[0;31m         return to_parquet(\n\u001b[0m\u001b[1;32m   2678\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2679\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/quakeflow/lib/python3.9/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mpartition_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFilePathOrBuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/quakeflow/lib/python3.9/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n - \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;34m\"Unable to find a usable engine; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;34m\"tried using: 'pyarrow', 'fastparquet'.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "station_locs.to_parquet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quakeflow",
   "language": "python",
   "name": "quakeflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
